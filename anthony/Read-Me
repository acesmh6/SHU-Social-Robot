All functions can be launch at once using roslaunch anthony all.launch (including alireza, social robot, but not preacher)

<<Face Recognition>>
Face recognition allows the robot to recognize someone who kicked it. The user can apologize by saying "I'm sorry" or "I apologize"
Use : roslaunch face_rec.launch
Warning : this launch file launch camera node, remove the line if needed
Note : if the face recognition have issues (displaying errors such as Database is out of date) :
- delete all entries in alireza/data/badpersons.txt (do not leave any character including carrier return)
- delete all entries in face_recognition/train.txt (same)
- delete the file face_recognition/facedata.xml
- launch face_rec.launch. The database will be clean and ready to use

<<Face Detector>>
This face detector subscribe to ros camera topic, and perform face detection. It publishes two messages, one including the biggest face informations, the other all detected faces informations from the camera frame. These informations are rectangles size and location on frame.
Use : rosrun anthony face_detector_node
Topics suscribed : /usb_cam/image_raw (Camera)
Topics published : singleface (Rectangle properties of the biggest detected face)
									 allfaces (Rectangle properties of all faces)

The camera node must be active.
The camera window feedback can be hidden commenting the following line :
#define DISPLAY_CAM
behaviour_handler use it to perform follow action (using vocal command "follow me")

Note : bump detection via odometry is currently deactivated (for others functionalities development), to activate it, uncomment in behaviour handler :
//rollpitch_sub = nh.subscribe<social_robot::RollPitch>("/roll_pitch",5,&BettyClass::rollpitchCB,this); 



<<Touchscreen reader>>
This node publish touch screen datas whenever available. It includes touch coordinates and touch state (someone is touching the screen or not). An event type is provided to differentiate new data as coordinate or as touch state.
Included in : touchscreen_command.launch
Topics suscribed : none
Topics published : touch_in (X Y coordinates, Touch state, Event type)

The touch screen device is read directly through input events :
			#define EVENT_DEVICE    "/dev/input/by-id/usb-TPK_USA_LLC_Touch_Fusion_4.-event-if00"

Warning : in case you are using multiple monitor (touch screen and external monitor for example), the touch screen might be responding with an offset (doing a rectangle selection on desktop shows it clearly). This might happen because the touch function is mapped to the wrong monitor. Displays your connected monitors with "xandr", and connect the correct one using, for example (replace the id with your touchscreen one, and the HDMI1 to another number or VGA) :
xinput map-to-output $(xinput list --id-only "TPK USA LLC Touch Fusion 4.") HDMI1.
If the problem is solved, add the previous line to your ~/.bashrc file so each time you open a terminal the remap is performed.

Use rostopic echo touch_in to check touch screen is being read and message sent by using it



<<Touch command recognition>>
The touch command recognition read through touch_in topic to record user tracks on touch screen. These samples are compaired to models automatically generated from a .bmp file bank (Models folder).
Included in : touchscreen_command.launch
Topics suscribed : touch_in (X Y coordinates, Touch state, Event type)
Topics published : touch_cmd (command name, command id)

The touch command recognition needs the touchscreen reader node to be active to perform recognition. This node open a QT window which should be settled on touch screen (it uses global coordinates to display), preferably in full screen for best confort. The recognition hovewer does not need this window (which only serves as a visual feedback). Models are automatically generated by .bmp file reading. When a command is recognized as a Model a message is published containing command name (.bmp file name without extension) and command id.

To add a new command, create a .bmp file, draw a dark track on a clear background, save it in Models folder. The node take care of using it for a Model.
The picture size is not important (models are normalized), neither the colors (just make sure every pixel supposed to be the track have a dark color and the background a clear one). It is advised to draw on the .bmp file with a 1-sized brush as every dark pixel is a track step and can augment compute time (using N-sized brush will lead to N-time usual compute time).

To remove a command, remove the .bmp file from Models folder.
behaviour_handler read touch_cmd topic and react to recognized commands.

Use of both previous node : roslaunch anthony touchscreen_command.launch

The list of current models are (in anthony/Models folder): 
- enable (start touchscreen commands, must be used first before others commands to activate the function)
- backward (behaviour handler makes robot move backward)
- forward, left, right (same principle)
- disable, disable2 (stop touch command reading by behaviour handler)
- tickle (not used for now)

If the sample is too short, the robot will ignore it AND stop current action. A quick touch is therefore the best way to stop the robot doing current performed action. The robot will not perform commands (even after enable have been recognised) if the power cable is plugged in.



<<Wifi command>>
Wifi command read commands sent from an external client on Wifi, and send them datas about the state of the robot (battery, sonars, ...).

Both the Social Robot Android APK and its source code are located in catkin_ws/src/wifi/APK (made for Visual Studio project implementing Xamarin for Android, make sure to install both if you want to modify the application. Xamarin packages are not given, if Xamarin is installed Visual Studio nuGet should import those automatically to modify sources or rebuild)

Use:

- (client) install Social Robot APK on an Android Device (made for 6.0 Marshmallow and tested on it, should be compatible with 4.4 Kit kat and above) or on an Android virtual machine/app player compatible with Wifi (Tested and working on Nox App Player for Windows and Genymotion for Ubuntu (personal use version))

- (server) connect to private Wifi Hotspot social on this robot :
In order to use Wifi command, the Wifi connection must be settled to "social-robot" on this robot. This can be accessed via Wifi icon > Connect to hidden Wi-fi Network > Connection > Choose social-robot. Once settled, the external device must connect to it. The password is robot4114 . Make sure both device are connected to this network to use Wifi command. This wifi network is not connected to internet, it just serves as p2p communication.

- connect the client device to this robot (password robot4114)

- rosrun wifi wifi_node (and behaviour handler)

- launch application and click on Connect to Robot (verify robot IP if needed). If the application is getting stucked (~10 seconds), your device might be connected to another wifi (stucking on SHU-USS, for example). The application gives informations about your Wifi each time you try to connect. If the connection is successful, the Wifi logo should become blue, and the robot will be ready to act.

Control commands are given as a sample of possible applications (move the robot, stop it, trigger follow face command if face detection node/camera is active. One press is enough the user do not need to keep his finger on the screen). The robot will not react if the power cable is plugged in.

Monitor screen gives a sample of datas which can be monitored, such as Battery state or Sonars states. Sonars proximity of an obstacle follow a color code :
- white : no data received yet
- close : Black <<<<<>>>>> Red : far
